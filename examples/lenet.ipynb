{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook demonstrates the LeNet model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load some dependencies for our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from logistic_sgd import LogisticRegression\n",
    "from mlp import HiddenLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start to define the actual convolution code.  We start by defining an object that represents a single layer of convolution that does the actual convolution operation followed by pooling over the output of that convolution.  These layers will be stacked in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from theano.tensor.signal import downsample\n",
    "from theano.tensor.nnet import conv\n",
    "\n",
    "class LeNetConvPoolLayer(object):\n",
    "    def __init__(self, rng, input, filter_shape, image_shape, poolsize=(2, 2)):\n",
    "        assert image_shape[1] == filter_shape[1]\n",
    "        self.input = input\n",
    "\n",
    "        # there are \"num input feature maps * filter height * filter width\"\n",
    "        # inputs to each hidden unit\n",
    "        fan_in = numpy.prod(filter_shape[1:])\n",
    "        # each unit in the lower layer receives a gradient from:\n",
    "        # \"num output feature maps * filter height * filter width\" / pooling size\n",
    "        fan_out = (filter_shape[0] * numpy.prod(filter_shape[2:]) /\n",
    "                   numpy.prod(poolsize))\n",
    "        # initialize weights with random weights\n",
    "        W_bound = numpy.sqrt(6. / (fan_in + fan_out))\n",
    "        self.W = theano.shared(\n",
    "            numpy.asarray(\n",
    "                rng.uniform(low=-W_bound, high=W_bound, size=filter_shape),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            borrow=True\n",
    "        )\n",
    "\n",
    "        # the bias is a 1D tensor -- one bias per output feature map\n",
    "        b_values = numpy.zeros((filter_shape[0],), dtype=theano.config.floatX)\n",
    "        self.b = theano.shared(value=b_values, borrow=True)\n",
    "\n",
    "        # convolve input feature maps with filters\n",
    "        conv_out = conv.conv2d(\n",
    "            input=input,\n",
    "            filters=self.W,\n",
    "            filter_shape=filter_shape,\n",
    "            image_shape=image_shape\n",
    "        )\n",
    "\n",
    "        # downsample each feature map individually, using maxpooling\n",
    "        pooled_out = downsample.max_pool_2d(\n",
    "            input=conv_out,\n",
    "            ds=poolsize,\n",
    "            ignore_border=True\n",
    "        )\n",
    "\n",
    "        # add the bias term. Since the bias is a vector (1D array), we first\n",
    "        # reshape it to a tensor of shape (1, n_filters, 1, 1). Each bias will\n",
    "        # thus be broadcasted across mini-batches and feature map\n",
    "        # width & height\n",
    "        self.output = T.tanh(pooled_out + self.b.dimshuffle('x', 0, 'x', 'x'))\n",
    "\n",
    "        # store parameters of this layer\n",
    "        self.params = [self.W, self.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next method uses the convolution layer above to make a stack of them and adds a hidden layer followed by a logistic regression classification layer on top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import fuel\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme\n",
    "from fuel.transformers import Cast\n",
    "\n",
    "fuel.config.floatX = theano.config.floatX = 'float32'\n",
    "\n",
    "\n",
    "def evaluate_lenet5(train, test, valid,\n",
    "                    learning_rate=0.1, n_epochs=200,\n",
    "                    nkerns=[20, 50], batch_size=500):\n",
    "    rng = numpy.random.RandomState(23455)\n",
    "\n",
    "    train_stream = DataStream.default_stream(\n",
    "        train, iteration_scheme=SequentialScheme(train.num_examples,\n",
    "                                                 batch_size))\n",
    "    valid_stream = DataStream.default_stream(\n",
    "        valid, iteration_scheme=SequentialScheme(valid.num_examples,\n",
    "                                                 batch_size))\n",
    "    test_stream = DataStream.default_stream(\n",
    "        test, iteration_scheme=SequentialScheme(test.num_examples,\n",
    "                                                batch_size))\n",
    "\n",
    "    x = T.tensor4('x')\n",
    "    yi = T.imatrix('y')\n",
    "    y = yi.reshape((yi.shape[0],))\n",
    "\n",
    "    # Construct the first convolutional pooling layer:\n",
    "    # filtering reduces the image size to (28-5+1 , 28-5+1) = (24, 24)\n",
    "    # maxpooling reduces this further to (24/2, 24/2) = (12, 12)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[0], 12, 12)\n",
    "    layer0 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=x,\n",
    "        image_shape=(batch_size, 1, 28, 28),\n",
    "        filter_shape=(nkerns[0], 1, 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    # Construct the second convolutional pooling layer\n",
    "    # filtering reduces the image size to (12-5+1, 12-5+1) = (8, 8)\n",
    "    # maxpooling reduces this further to (8/2, 8/2) = (4, 4)\n",
    "    # 4D output tensor is thus of shape (batch_size, nkerns[1], 4, 4)\n",
    "    layer1 = LeNetConvPoolLayer(\n",
    "        rng,\n",
    "        input=layer0.output,\n",
    "        image_shape=(batch_size, nkerns[0], 12, 12),\n",
    "        filter_shape=(nkerns[1], nkerns[0], 5, 5),\n",
    "        poolsize=(2, 2)\n",
    "    )\n",
    "\n",
    "    # the HiddenLayer being fully-connected, it operates on 2D matrices of\n",
    "    # shape (batch_size, num_pixels) (i.e matrix of rasterized images).\n",
    "    # This will generate a matrix of shape (batch_size, nkerns[1] * 4 * 4),\n",
    "    # or (500, 50 * 4 * 4) = (500, 800) with the default values.\n",
    "    layer2_input = layer1.output.flatten(2)\n",
    "\n",
    "    # construct a fully-connected sigmoidal layer\n",
    "    layer2 = HiddenLayer(\n",
    "        rng,\n",
    "        input=layer2_input,\n",
    "        n_in=nkerns[1] * 4 * 4,\n",
    "        n_out=500,\n",
    "        activation=T.tanh\n",
    "    )\n",
    "\n",
    "    # classify the values of the fully-connected sigmoidal layer\n",
    "    layer3 = LogisticRegression(input=layer2.output, n_in=500, n_out=10)\n",
    "\n",
    "    # the cost we minimize during training is the NLL of the model\n",
    "    cost = layer3.negative_log_likelihood(y)\n",
    "\n",
    "    # create a function to compute the mistakes that are made by the model\n",
    "    model_errors = theano.function(\n",
    "        [x, yi],\n",
    "        layer3.errors(y)\n",
    "    )\n",
    "\n",
    "    # create a list of all model parameters to be fit by gradient descent\n",
    "    params = layer3.params + layer2.params + layer1.params + layer0.params\n",
    "\n",
    "    # create a list of gradients for all model parameters\n",
    "    grads = T.grad(cost, params)\n",
    "\n",
    "    # train_model is a function that updates the model parameters by\n",
    "    # SGD Since this model has many parameters, it would be tedious to\n",
    "    # manually create an update rule for each model parameter. We thus\n",
    "    # create the updates list by automatically looping over all\n",
    "    # (params[i], grads[i]) pairs.\n",
    "    updates = [\n",
    "        (param_i, param_i - learning_rate * grad_i)\n",
    "        for param_i, grad_i in zip(params, grads)\n",
    "    ]\n",
    "\n",
    "    train_model = theano.function(\n",
    "        [x, yi],\n",
    "        cost,\n",
    "        updates=updates\n",
    "    )\n",
    "\n",
    "    # early-stopping parameters\n",
    "    patience = 10000  # look as this many examples regardless\n",
    "    patience_increase = 2  # wait this much longer when a new best is found\n",
    "\n",
    "    # a relative improvement of this much is considered significant\n",
    "    improvement_threshold = 0.995\n",
    "\n",
    "    n_train_batches = (train.num_examples + batch_size - 1) // batch_size\n",
    "    \n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    best_iter = 0\n",
    "    test_score = 0.\n",
    "    start_time = time.clock()\n",
    "\n",
    "    epoch = 0\n",
    "    iter = 0\n",
    "    done_looping = False\n",
    "\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "\n",
    "        minibatch_index = 0\n",
    "        for minibatch in train_stream.get_epoch_iterator():\n",
    "            iter += 1\n",
    "            minibatch_index += 1\n",
    "            if iter % 100 == 0:\n",
    "                print('training @ iter = ', iter)\n",
    "\n",
    "            error = train_model(minibatch[0], minibatch[1])\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                # compute zero-one loss on validation set\n",
    "                validation_losses = [model_errors(vb[0], vb[1]) for vb\n",
    "                                     in valid_stream.get_epoch_iterator()]\n",
    "                this_validation_loss = numpy.mean(validation_losses)\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' %\n",
    "                      (epoch, minibatch_index + 1, n_train_batches,\n",
    "                       this_validation_loss * 100.))\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    # improve patience if loss improvement is good enough\n",
    "                    if this_validation_loss < best_validation_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = [\n",
    "                        model_errors(tb[0], tb[1])\n",
    "                        for tb in test_stream.get_epoch_iterator()\n",
    "                    ]\n",
    "                    test_score = numpy.mean(test_losses)\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                           test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = time.clock()\n",
    "    print('Optimization complete.')\n",
    "    print('Best validation score of %f %% obtained at iteration %i, '\n",
    "          'with test performance %f %%' %\n",
    "          (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print('The code ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "\n",
    "    # This is to make the pretty pictures in the cells below\n",
    "    layer0_out = theano.function([x], layer0.output)\n",
    "    layer1_out = theano.function([x], layer1.output)\n",
    "    \n",
    "    return params, layer0_out, layer1_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell runs the model and allows you to play with a few hyperparameters.  The ones below take about 1 to 2 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from fuel.datasets import MNIST\n",
    "\n",
    "train = MNIST(which_sets=('train',), subset=slice(0, 50000))\n",
    "valid = MNIST(which_sets=('train',), subset=slice(50000, 60000))\n",
    "test = MNIST(which_sets=('test',))\n",
    "\n",
    "params, layer0_out, layer1_out = evaluate_lenet5(train, test, valid,\n",
    "                                                 learning_rate=0.1, n_epochs=10,\n",
    "                                                 nkerns=[10, 25], batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most convolution model it can be interesting to show what the trained filters look like.  The code below does that from the parameters returned by the training function above.  In this model there isn't much of an effect since the filters are 5x5 and we can't see much unfortunately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import tile_raster_images\n",
    "\n",
    "filts1 = params[6].get_value()\n",
    "filts2 = params[4].get_value()\n",
    "\n",
    "plt.clf()\n",
    "\n",
    "# Increase the size of the figure\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "\n",
    "# Make a grid for the two layers\n",
    "gs = plt.GridSpec(1, 2, width_ratios=[1, 25], height_ratios=[1, 1])\n",
    "a = plt.subplot(gs[0])\n",
    "b = plt.subplot(gs[1])\n",
    "\n",
    "\n",
    "# Show the first layer filters (the small column)\n",
    "a.imshow(tile_raster_images(filts1.reshape(10, 25), img_shape=(5, 5), tile_shape=(10, 1), tile_spacing=(1,1)),\n",
    "           cmap=\"Greys\", interpolation=\"none\")\n",
    "a.axis('off')\n",
    "\n",
    "# Show the second layer filters (the large block)\n",
    "b.imshow(tile_raster_images(filts2.reshape(250, 25), img_shape=(5, 5), tile_shape=(10, 25), tile_spacing=(1,1)),\n",
    "           cmap=\"Greys\", interpolation=\"none\")\n",
    "b.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can also be interesting is to draw the outputs of the filters for an example.  This works somewhat better for this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import tile_raster_images\n",
    "\n",
    "# Grab some input examples from the test set (we cheat a bit here)\n",
    "sample = test.get_data(None, slice(0, 50))[0]\n",
    "# We will print this example amongst the batch\n",
    "example = 7\n",
    "\n",
    "plt.gcf()\n",
    "\n",
    "# Increase the size of the figure\n",
    "plt.gcf().set_size_inches(15, 10)\n",
    "\n",
    "gs = plt.GridSpec(1, 3, width_ratios=[1, 1, 1], height_ratios=[1, 1, 1])\n",
    "\n",
    "# Draw the input data\n",
    "a = plt.subplot(gs[0])\n",
    "a.imshow(sample[example, 0], cmap=\"Greys\", interpolation='none')\n",
    "a.axis('off')\n",
    "\n",
    "# Compute first layer output\n",
    "out0 = layer0_out(sample)[example]\n",
    "\n",
    "# Draw its output\n",
    "b = plt.subplot(gs[1])\n",
    "b.imshow(tile_raster_images(out0.reshape(10, 144), img_shape=(12, 12), tile_shape=(5, 2), tile_spacing=(1, 1)),\n",
    "         cmap=\"Greys\", interpolation='none')\n",
    "b.axis('off')\n",
    "\n",
    "# Compute the second layer output\n",
    "out1 = layer1_out(sample)[example]\n",
    "\n",
    "# Draw it\n",
    "c = plt.subplot(gs[2])\n",
    "c.imshow(tile_raster_images(out1.reshape(25, 16), img_shape=(4, 4), tile_shape=(5, 5), tile_spacing=(1, 1)),\n",
    "         cmap=\"Greys\", interpolation='none')\n",
    "c.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some things you can try with this model:\n",
    "- change the non linearity of the convolution to rectifier unit.\n",
    "- add an extra mlp layer.\n",
    "\n",
    "If you break the code too much you can get back to the working initial code by loading the lenet.py file with the cell below. (Or just reset the git repo ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load lenet.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
